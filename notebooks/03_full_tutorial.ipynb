{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Tutorial: Topic Modeling Workflow\n",
    "\n",
    "This notebook demonstrates a complete topic modeling workflow from data preparation to result analysis.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Data Loading** - Load and explore raw text data\n",
    "2. **Preprocessing** - Clean and tokenize documents\n",
    "3. **Vocabulary** - Build vocabulary with filtering\n",
    "4. **Model Selection** - Choose and configure model\n",
    "5. **Training** - Train with monitoring\n",
    "6. **Evaluation** - Compute metrics\n",
    "7. **Visualization** - Explore results\n",
    "8. **Saving** - Export model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NBTM imports\n",
    "from nbtm.data import (\n",
    "    Corpus,\n",
    "    load_corpus,\n",
    "    TextPreprocessor,\n",
    "    Vocabulary,\n",
    "    ENGLISH_STOPWORDS,\n",
    ")\n",
    "from nbtm.models import create_model, get_available_models\n",
    "from nbtm.training import Trainer\n",
    "from nbtm.evaluation import (\n",
    "    compute_coherence,\n",
    "    compute_topic_diversity,\n",
    "    compute_perplexity,\n",
    ")\n",
    "from nbtm.visualization import (\n",
    "    plot_topic_words,\n",
    "    plot_topic_heatmap,\n",
    "    plot_document_topics,\n",
    "    plot_topic_distribution,\n",
    "    plot_training_history,\n",
    "    plot_topic_wordcloud,\n",
    "    plot_all_topic_wordclouds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "For this tutorial, we'll create a synthetic corpus. In practice, you would load your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic document collection (simulating academic abstracts)\n",
    "raw_documents = [\n",
    "    # Machine Learning topic\n",
    "    \"Machine learning algorithms enable computers to learn patterns from data without explicit programming.\",\n",
    "    \"Deep neural networks have revolutionized computer vision and natural language processing tasks.\",\n",
    "    \"Supervised learning requires labeled training data to learn a mapping from inputs to outputs.\",\n",
    "    \"Gradient descent optimization is fundamental to training neural network models.\",\n",
    "    \"Convolutional neural networks excel at image recognition and computer vision applications.\",\n",
    "    \"Recurrent neural networks process sequential data like text and time series.\",\n",
    "    \"Transfer learning enables models to leverage knowledge from pre-trained networks.\",\n",
    "    \"Regularization techniques prevent overfitting in machine learning models.\",\n",
    "    \n",
    "    # Statistics topic\n",
    "    \"Bayesian inference provides a principled framework for updating beliefs given new evidence.\",\n",
    "    \"Hypothesis testing allows researchers to make decisions based on statistical evidence.\",\n",
    "    \"The central limit theorem states that sample means approach a normal distribution.\",\n",
    "    \"Maximum likelihood estimation finds parameter values that maximize data probability.\",\n",
    "    \"Confidence intervals quantify uncertainty in parameter estimates.\",\n",
    "    \"Regression analysis models relationships between dependent and independent variables.\",\n",
    "    \"The bootstrap method estimates sampling distributions through resampling.\",\n",
    "    \"Markov chain Monte Carlo enables sampling from complex probability distributions.\",\n",
    "    \n",
    "    # NLP topic\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Word embeddings represent words as dense vectors capturing semantic meaning.\",\n",
    "    \"Transformer models have achieved state-of-the-art results in NLP tasks.\",\n",
    "    \"Named entity recognition identifies and classifies entities in text.\",\n",
    "    \"Sentiment analysis determines the emotional tone of text documents.\",\n",
    "    \"Machine translation converts text from one language to another.\",\n",
    "    \"Text classification assigns categories to documents based on content.\",\n",
    "    \"Language models predict the probability of word sequences.\",\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(raw_documents)} documents\")\n",
    "print(f\"\\nSample document:\\n{raw_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Clean and tokenize the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_numbers=True,\n",
    "    remove_stopwords=True,\n",
    "    stopwords=ENGLISH_STOPWORDS,\n",
    "    min_word_length=3,\n",
    ")\n",
    "\n",
    "# Preprocess documents\n",
    "documents = [preprocessor.process(doc) for doc in raw_documents]\n",
    "\n",
    "print(f\"Original: {raw_documents[0]}\")\n",
    "print(f\"\\nProcessed: {documents[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore document statistics\n",
    "doc_lengths = [len(doc) for doc in documents]\n",
    "\n",
    "print(f\"Document statistics:\")\n",
    "print(f\"  Total documents: {len(documents)}\")\n",
    "print(f\"  Min length: {min(doc_lengths)} words\")\n",
    "print(f\"  Max length: {max(doc_lengths)} words\")\n",
    "print(f\"  Mean length: {np.mean(doc_lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Vocabulary\n",
    "\n",
    "Create a vocabulary with word frequency filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab = Vocabulary()\n",
    "vocab.build_from_documents(\n",
    "    documents,\n",
    "    min_df=2,  # Minimum document frequency\n",
    "    max_df_ratio=0.8,  # Maximum document frequency ratio\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nTop 20 words by frequency:\")\n",
    "for word, count in vocab.get_top_words(20):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "Choose and configure the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "print(\"Available models:\")\n",
    "for name, desc in get_available_models().items():\n",
    "    print(f\"  {name}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(\n",
    "    \"lda_gibbs\",\n",
    "    num_topics=3,\n",
    "    alpha=0.1,      # Document-topic prior (lower = sparser)\n",
    "    beta=0.01,      # Topic-word prior (lower = sparser)\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Type: {model.__class__.__name__}\")\n",
    "print(f\"  Topics: {model.num_topics}\")\n",
    "print(f\"  Alpha: {model.alpha}\")\n",
    "print(f\"  Beta: {model.beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Train the model with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Training model...\")\n",
    "model.fit(\n",
    "    documents,\n",
    "    num_iterations=500,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final log-likelihood: {model.log_likelihood():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if model.training_history:\n",
    "    fig = plot_training_history(model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "coherence_umass = compute_coherence(model, documents, measure=\"umass\")\n",
    "diversity = compute_topic_diversity(model, top_n=10)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"  Topic Coherence (UMass): {coherence_umass:.4f}\")\n",
    "print(f\"  Topic Diversity: {diversity:.4f}\")\n",
    "print(f\"  Number of Topics: {model.num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation guide\n",
    "print(\"\\nMetric Interpretation:\")\n",
    "print(\"  Coherence: Higher is better (less negative for UMass)\")\n",
    "print(\"  Diversity: Higher is better (0-1, measures topic uniqueness)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Explore the learned topics visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics\n",
    "print(\"Learned Topics:\")\n",
    "print(\"=\" * 60)\n",
    "model.print_topics(top_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-word bar chart\n",
    "fig = plot_topic_words(model, top_n=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-word heatmap\n",
    "fig = plot_topic_heatmap(model, top_n=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds for each topic\n",
    "fig = plot_all_topic_wordclouds(model, ncols=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-topic distribution\n",
    "fig = plot_document_topics(model, doc_indices=list(range(10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic distribution across corpus\n",
    "fig = plot_topic_distribution(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Specific Documents\n",
    "\n",
    "Examine topic assignments for individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document-topic matrix\n",
    "doc_topics = model.get_document_topics()\n",
    "\n",
    "# Analyze a few documents\n",
    "for i in [0, 8, 16]:  # One from each topic\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"  Text: {raw_documents[i][:80]}...\")\n",
    "    print(f\"  Topic distribution: {doc_topics[i]}\")\n",
    "    dominant_topic = np.argmax(doc_topics[i])\n",
    "    print(f\"  Dominant topic: {dominant_topic} ({doc_topics[i][dominant_topic]:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Load Model\n",
    "\n",
    "Export the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = \"outputs/full_tutorial_model.pkl\"\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from nbtm.models import GibbsLDA\n",
    "\n",
    "loaded_model = GibbsLDA.load(model_path)\n",
    "print(f\"Loaded model: {loaded_model}\")\n",
    "print(f\"\\nVerify topics match:\")\n",
    "loaded_model.print_topics(top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference on New Documents\n",
    "\n",
    "Apply the trained model to new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New documents\n",
    "new_docs_raw = [\n",
    "    \"Neural networks learn hierarchical representations of data.\",\n",
    "    \"Statistical hypothesis testing requires careful consideration of p-values.\",\n",
    "]\n",
    "\n",
    "# Preprocess\n",
    "new_docs = [preprocessor.process(doc) for doc in new_docs_raw]\n",
    "\n",
    "# Infer topics\n",
    "new_doc_topics = model.transform(new_docs)\n",
    "\n",
    "for i, (doc, topics) in enumerate(zip(new_docs_raw, new_doc_topics)):\n",
    "    print(f\"\\nNew Document {i+1}:\")\n",
    "    print(f\"  Text: {doc}\")\n",
    "    print(f\"  Topics: {topics}\")\n",
    "    print(f\"  Dominant: Topic {np.argmax(topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered the complete topic modeling workflow:\n",
    "\n",
    "1. **Data Loading** - Load raw text documents\n",
    "2. **Preprocessing** - Clean and tokenize with `TextPreprocessor`\n",
    "3. **Vocabulary** - Build filtered vocabulary with `Vocabulary`\n",
    "4. **Model** - Create model with `create_model()`\n",
    "5. **Training** - Train with `model.fit()`\n",
    "6. **Evaluation** - Compute coherence and diversity\n",
    "7. **Visualization** - Plot topics with various visualizations\n",
    "8. **Saving** - Export model with `model.save()`\n",
    "9. **Inference** - Apply to new documents with `model.transform()`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different numbers of topics\n",
    "- Try different models (HDP for automatic topic selection)\n",
    "- Fine-tune hyperparameters (alpha, beta)\n",
    "- Use CLI for batch experiments: `nbtm train --config config.yaml`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
