{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "This notebook demonstrates the basic usage of NBTM for topic modeling.\n",
    "\n",
    "## Contents\n",
    "1. Create a topic model\n",
    "2. Prepare sample data\n",
    "3. Train the model\n",
    "4. View results\n",
    "5. Basic visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nbtm.models import create_model, get_available_models\n",
    "from nbtm.data import Corpus, TextPreprocessor\n",
    "from nbtm.evaluation import compute_coherence, compute_topic_diversity\n",
    "from nbtm.visualization import plot_topic_words, plot_topic_wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Available Models\n",
    "\n",
    "Check what models are available in NBTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "print(\"Available models:\")\n",
    "for name, desc in get_available_models().items():\n",
    "    print(f\"  - {name}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data\n",
    "\n",
    "We'll create sample documents for demonstration. In practice, you would load your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents (already tokenized)\n",
    "documents = [\n",
    "    [\"machine\", \"learning\", \"algorithm\", \"data\", \"model\", \"training\"],\n",
    "    [\"deep\", \"learning\", \"neural\", \"network\", \"layer\", \"training\"],\n",
    "    [\"python\", \"programming\", \"code\", \"function\", \"class\", \"module\"],\n",
    "    [\"data\", \"analysis\", \"statistics\", \"probability\", \"distribution\"],\n",
    "    [\"neural\", \"network\", \"deep\", \"learning\", \"optimization\", \"gradient\"],\n",
    "    [\"bayesian\", \"inference\", \"prior\", \"posterior\", \"probability\", \"distribution\"],\n",
    "    [\"topic\", \"model\", \"document\", \"word\", \"distribution\", \"lda\"],\n",
    "    [\"machine\", \"learning\", \"classification\", \"regression\", \"prediction\"],\n",
    "    [\"programming\", \"python\", \"library\", \"package\", \"development\"],\n",
    "    [\"statistics\", \"hypothesis\", \"testing\", \"confidence\", \"interval\"],\n",
    "    [\"natural\", \"language\", \"processing\", \"text\", \"word\", \"embedding\"],\n",
    "    [\"optimization\", \"gradient\", \"descent\", \"loss\", \"function\", \"convergence\"],\n",
    "]\n",
    "\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(f\"Sample document: {documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Train Model\n",
    "\n",
    "Create a Gibbs Sampling LDA model and train it on our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(\n",
    "    \"lda_gibbs\",\n",
    "    num_topics=3,\n",
    "    alpha=0.1,\n",
    "    beta=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(documents, num_iterations=500)\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Final log-likelihood: {model.log_likelihood():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "Examine the learned topics and document-topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics\n",
    "print(\"Learned Topics:\")\n",
    "print(\"=\" * 50)\n",
    "model.print_topics(top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic words programmatically\n",
    "topics = model.get_all_topic_words(top_n=5)\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    words = [word for word, prob in topic]\n",
    "    print(f\"Topic {i}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document-topic distributions\n",
    "doc_topics = model.get_document_topics()\n",
    "\n",
    "print(f\"Document-topic matrix shape: {doc_topics.shape}\")\n",
    "print(f\"\\nFirst document topic distribution: {doc_topics[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Evaluate the model using coherence and diversity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coherence\n",
    "coherence = compute_coherence(model, documents, measure=\"umass\")\n",
    "print(f\"Topic Coherence (UMass): {coherence:.4f}\")\n",
    "\n",
    "# Compute diversity\n",
    "diversity = compute_topic_diversity(model, top_n=10)\n",
    "print(f\"Topic Diversity: {diversity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Visualize the learned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topic words\n",
    "fig = plot_topic_words(model, top_n=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud for a topic\n",
    "fig = plot_topic_wordcloud(model, topic_id=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Load Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"outputs/quickstart_model.pkl\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load model\n",
    "from nbtm.models import GibbsLDA\n",
    "loaded_model = GibbsLDA.load(\"outputs/quickstart_model.pkl\")\n",
    "print(f\"Model loaded: {loaded_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See `02_model_comparison.ipynb` for comparing different models\n",
    "- See `03_full_tutorial.ipynb` for a complete workflow with real data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
